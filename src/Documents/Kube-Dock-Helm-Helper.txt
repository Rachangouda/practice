------------------------
 Kubernitees helpers
-------------------
-------------------------------
install specific version
-------------------------------
curl -LO curl -LO https://storage.googleapis.com/kubernetes-release/release/v1.13.4/bin/linux/amd64/kubectl && chmod +x ./kubectl && sudo mv ./kubectl /usr/bin/kubectl
curl -LO curl -LO https://storage.googleapis.com/kubernetes-release/release/v1.13.4/bin/linux/amd64/kubeadm && chmod +x ./kubeadm && sudo mv ./kubeadm /usr/bin/kubeadm
curl -LO curl -LO https://storage.googleapis.com/kubernetes-release/release/v1.13.4/bin/linux/amd64/kubelet && chmod +x ./kubelet && sudo mv ./kubelet /usr/bin/kubelet
curl -Lo minikube https://storage.googleapis.com/minikube/releases/v0.28.2/minikube-linux-amd64 && chmod +x minikube && sudo mv minikube /usr/local/bin/
------------------------------------------------------------------------
minikube start/status/stop
kubectl version
kubectl cluster-info #master details
kubectl get nodes # show all nodes 
kubectl get pod -n analytics
#run app on kubernetes
kubectl run kubernetes-bootcamp --image=gcr.io/google-samples/kubernetes-bootcamp:v1 --port=8080
#To list your deployments
kubectl get deployments
# The kubectl command can create a proxy that will forward communications into the cluster-wide, private network. The proxy can be terminated by pressing control-C and won't show any output while its running.
kubectl proxy
kubectl get - list resources
kubectl describe - show detailed information about a resource
kubectl logs - print the logs from a container in a pod
kubectl exec - execute a command on a container in a pod

#list pods configmaps
kubectl get configmaps -n analytics
#print configuration
kubectl get configmaps ingress-controller-leader-ipfix-0-nginx -o yaml
#edit config then pod will be recreated
kubectl edit configmaps ingress-controller-leader-ipfix-0-nginx -o yaml -n analytics
#/etc/ngnix/nginx.conf where value will be present for template

#to save config of full system
kubectl get configmaps -o yaml -n analytics

#Service is load-balancing the traffic. To find out the exposed IP and Port 
kubectl describe services/kubernetes-bootcamp

#Run a Hello World application in your cluster
kubectl run hello-world --replicas=5 --labels="run=load-balancer-example" --image=gcr.io/google-samples/node-hello:1.0  --port=8080

#Create a Service object that exposes the deployment:
kubectl expose deployment hello-world --type=LoadBalancer --name=my-service

#Display detailed information about the Service:
kubectl get services my-service
kubectl describe services my-service

#
kubectl delete services my-service
kubectl delete deployment hello-world

#forcefully delete pod 
kubectl delete pods <pod> --grace-period=20 --force

#get the IP address and port for the Service
minikube service <servicename> --url

#Monitor commands
kubectl get events --namespace=my-namespace
kubectl describe node  <node-name>
kubectl get node <node-name> -o yaml
#see node selector label
kubectl get nodes --show-labels
#list all containers in kubernetes
kubectl get pods --all-namespaces -o jsonpath="{..image}" |\
tr -s '[[:space:]]' '\n' |\
sort |\
uniq -c

#list container by pod
kubectl get pods --all-namespaces -o=jsonpath='{range .items[*]}{"\n"}{.metadata.name}{":\t"}{range .spec.containers[*]}{.image}{", "}{end}{end}' | sort

#List Containers filtering by Pod label
kubectl get pods --all-namespaces -o=jsonpath="{..image}" -l app=nginx

# pods are crashing 
#list etcd members 
etcdctl member list
etcdctl member remove fbdf406b970cb7fc if error in  journalctl -u etcd >> (ID mismatch got 7273776352ec5cc8 want b71a048acf578b73) like
etcdctl member add grey-manager-1 https://10.75.86.67:2380
#On failed node, remove etcd member data
cd /data0/etcd 
mv member/ bk.member
modify /etc/systemd/system/etcd.service, change " --initial-cluster-state new \ " to " --initial-cluster-state existing\ "
systemctl daemon-reload;systemctl restart etcd
#Check etcdc cluster health
etcdctl member list
---------------------------------------------------
check status of pods
----------------------------------------------------
kubectl describe pod  consul-0 --namespace analytics
kubectl get pod elastic-utils-nqgk9 -n analytics -o yaml | less
---------------------------------------------------
export POD_NAME=$(kubectl get pods -o go-template --template '{{range .items}}{{.metadata.name}}{{"\n"}}{{end}}')
echo Name of the Pod: $POD_NAME
To see the output of our application, run a curl request.

curl http://localhost:8001/api/v1/namespaces/default/pods/$POD_NAME/proxy/
kubectl exec $POD_NAME env

#find the ports 
kubectl get svc --all-namespaces -o go-template='{{range .items}}{{range.spec.ports}}{{if .nodePort}}{{.nodePort}}{{.}}{{"\n"}}{{end}}{{end}}{{end}}'

-----------------------
workaround for IP address not able to allocate not in range
----------------------
kubeadm reset
systemctl stop kubelet
systemctl stop docker
rm -rf /var/lib/cni/
rm -rf /var/lib/kubelet/*
rm -rf /etc/cni/
ifconfig cni0 down
ifconfig flannel.1 down
ifconfig docker0 down
ip link delete cni0
ip link delete flannel.1

then 
systemctl start docker && systemctl enable docker

#try rm -rf .kube if not works above 
---------------------------



=======================================================================================================================================================
Docker Helpers
=======================================================================================================================================================
----------------------------------------------------------
Docker helper commands
----------------------------------------------------------
yum install docker-1.13.1-103.git7f2769b.el7.x86_64
--verify the docker proxy settings in ./systemd/system/docker.service.d/http-proxy.conf file
systemctl show --property=Environment docker
systemctl restart docker
--create docker container volume 
docker run -p 80:80 -v /var/data imagename
--info about volume location and other info
docker inspect mycontainer
-------------
./dockerBuilder.sh --images-file test_images

#tag
docker tag elastic_sync:latest sandbox-docker-releases.repo.lab.pl.alcatel-lucent.com/master/elastic_sync:build_3003
docker tag snmp_keeper:latest bcmt-registry:5000/master/snmp_keeper:build_3007

docker tag house_keeper:latest sandbox-docker-releases.repo.lab.pl.alcatel-lucent.com/master/house_keeper:build_3020
docker tag elasticsearch:latest sandbox-docker-releases.repo.lab.pl.alcatel-lucent.com/master/elasticsearch:build_3020
docker tag elastic_utils:latest sandbox-docker-releases.repo.lab.pl.alcatel-lucent.com/master/elastic_utils:build_3020
docker tag fluentd:latest sandbox-docker-releases.repo.lab.pl.alcatel-lucent.com/master/fluentd:build_3009
docker tag kibana:latest sandbox-docker-releases.repo.lab.pl.alcatel-lucent.com/master/kibana:build_3009
----------------
#manual pull images
docker pull sandbox-docker-releases.repo.lab.pl.alcatel-lucent.com/master/nssbase/elastic_utils:build_3026
docker pull sandbox-docker-releases.repo.lab.pl.alcatel-lucent.com/master/nssbase/elasticsearch:build_3026
docker pull sandbox-docker-releases.repo.lab.pl.alcatel-lucent.com/master/fluentd:build_3025
docker pull sandbox-docker-releases.repo.lab.pl.alcatel-lucent.com/master/house_keeper:build_3025
docker pull sandbox-docker-releases.repo.lab.pl.alcatel-lucent.com/master/calm:build_3025
docker pull sandbox-docker-releases.repo.lab.pl.alcatel-lucent.com/master/alma_config:build_3017
docker pull sandbox-docker-releases.repo.lab.pl.alcatel-lucent.com/master/administration_services:build_3017
sandbox-docker-releases.repo.lab.pl.alcatel-lucent.com/master/keycloak:build_3022

#kibana base image builder 
go to charts/kibana folder 
docker build --no-cache -f Dockerfile -t sandbox-docker-releases.repo.lab.pl.alcatel-lucent.com/master/nssbase/kibana_base:18 . --build-arg https_proxy=http://10.144.1.10:8080                  --build-arg http_proxy=http://10.144.1.10:8080                  --build-arg no_proxy=10.75.159.218,192.168.0.1,localhost,127.0.0.1,*.nsn.com,*.nokia.com,*.alcatel-lucent.com,*.nsn-net.net,artifactory.devsec.eecloud.dynamic.nsn-net.net,docker.mbbsec.dev.eecloud.dynamic.nsn-net.net,lss-repo1.ih.lucent.com,repo.lab.pl.alcatel-lucent.com                  --build-arg PIP_INDEX_URL=https://repo.lab.pl.alcatel-lucent.com/api/pypi/python/simple                  --build-arg PIP_TRUSTED_HOST=repo.lab.pl.alcatel-lucent.com                  --build-arg ARTIFACTORY_ADDRESS=repo.lab.pl.alcatel-lucent.com -v /root/nss-kibana-rpmbuilder/build_artifacts:/kibana


# find ID of your running container:
docker ps
# create image (snapshot) from container filesystem
docker commit 12345678904b5 mysnapshot
# explore this filesystem using bash (for example)
docker run -t -i mysnapshot /bin/bash

#delete snapshot using (filesystem of the running container is not affected!):
docker rmi mysnapshot
#delete all images
docker rmi -f $(docker images -a -q)
docker system prune -a -f
docker rmi -f $(docker images | grep "build_3017" | awk '{print $3}')
#remove/delete by pattern column start with index 0 forcefully
docker images -a | grep "<none>" | awk '{print $3}' | xargs docker rmi -f
docker container ls -a
docker container rm cc3f2ff51cab cd20b396a061
#https://linuxize.com/post/how-to-remove-docker-images-containers-volumes-and-networks/
#To remove all stopped containers use the docker container prune command:
docker container prune

#delete all helms
helm ls --all | awk '{print $1}' | grep -v "NAME" | xargs helm del ; helm ls --all | awk '{print $1}' | grep -v "NAME" | xargs helm del –purge

  docker container exec
  docker ps -a | grep snmp
  docker exec -it e40f6bca6f4c bash
  docker run -it imagename
  docker run -p 80:80 imagename   ---<port for machine : port for container   >

#check docker state
docker inspect -f '{{.State.Running}}' $container_name

#docker image browse/see contents
docker run --rm -it elastic_utils sh
-----------------------------------
proxy settings
-------------------
cat <<EOF > /etc/systemd/system/docker.service.d/http-proxy.conf[Service]
Environment="HTTP_PROXY=http://inban1b-proxy.apac.nsn-net.net:8080"
Environment="HTTPS_PROXY=http://inban1b-proxy.apac.nsn-net.net:8080"
EOF
which proxy do you use now
try any of these,
export http_proxy="http://inban1b-proxy.apac.nsn-net.net:8080"
export http_proxy=http://proxy.lbs.alcatel-lucent.com:8000/
export https_proxy=https://proxy.lbs.alcatel-lucent.com:8000/
export http_proxy=http://inmum1a-proxy.apac.nsn-net.net:8080/
export HTTPS_PROXY=87.254.212.120:8080/
10.144.1.10
systemctl daemon-reload
systemctl restart docker
=======================================================================================================================================================
Helm Helpers
=======================================================================================================================================================
#list all running helm charts
helm ls -a 
helm delete --purge <chart-name>
#dependency update
helm dep up <foochart>
helm create mychart
helm package mychart #Archived mychart-0.1.-.tgz
#find issues with your chart’s formatting or information
helm lint mychart

-------------------------------------------------------------------
manual install pods
-------------------------------------------------------------------
helm install snmp-generator \
	--name snmp-generator \
	--values /root/nss-repo/deployment/k8s/global-values.yaml \
	--values /root/nss-repo/deployment/k8s/generators/snmp-generator/values.yaml \
	--values /root/nss-repo/deployment/k8s/generators/snmp-generator/data/snmp_values.yaml


helm install --name elastic-utils /etc/nss/configuration/nss-current/charts/elastic-utils --values /etc/nss/configuration/nss-current/global-values.yaml --values /etc/nss/configuration/nss-current/charts/elastic-utils/values.yaml --values /etc/nss/configuration/nss-current/charts/elastic-utils/overrides.yaml

#/etc/nss/configuration/nss-current/charts/kibana/screts/kibana.yml
#requestTimeout: 120000
helm delete --purge kibana;
helm install --name kibana /etc/nss/configuration/nss-current/charts/kibana --values /etc/nss/configuration/nss-current/global-values.yaml --values /etc/nss/configuration/nss-current/charts/kibana/values.yaml --values /etc/nss/configuration/nss-current/charts/kibana/overrides.yaml

citm-ingress port issue workaroud
/etc/nss/configuration/nss-current/charts/citm-ingress/templates/ipfix-daemonset.yaml #change --healthz-port
kubectl get ds -n analytics
kubectl edit ds/citm-ingress-controller-ipfix-0 -n analytics

helm delete --purge citm-ingress
helm install --name citm-ingress /etc/nss/configuration/nss-current/charts/citm-ingress --values /etc/nss/configuration/nss-current/global-values.yaml --values /etc/nss/configuration/nss-current/charts/citm-ingress/values.yaml --values /etc/nss/configuration/nss-current/charts/citm-ingress/overrides.yaml


helm install --name elasticsearch /etc/nss/configuration/nss-current/charts/elasticsearch --values /etc/nss/configuration/nss-current/global-values.yaml --values /etc/nss/configuration/nss-current/charts/elasticsearch/values.yaml --values /etc/nss/configuration/nss-current/charts/elasticsearch/overrides.yaml

 https://elasticsearch-http.analytics.svc.cluster.local:9200
ln -s -f /etc/nss/configuration/nss-master-build_3020_2019-09-09_14-29-31 /etc/nss/configuration/nss-current

helm install --name sg-config /etc/nss/configuration/nss-current/charts/sg-config --values /etc/nss/configuration/nss-current/global-values.yaml --values /etc/nss/configuration/nss-current/charts/sg-config/values.yaml --values /etc/nss/configuration/nss-current/charts/sg-config/overrides.yaml

helm install --name elastic-sync /etc/nss/configuration/nss-current/charts/elastic-sync --values /etc/nss/configuration/nss-current/global-values.yaml --values /etc/nss/configuration/nss-current/charts/elastic-sync/values.yaml --values /etc/nss/configuration/nss-current/charts/elastic-sync/overrides.yaml


 helm install --name postgres /etc/nss/configuration/nss-master-build_3030_2019-10-12_13-15-42/charts/postgres --values /etc/nss/configuration/nss-master-build_3030_2019-10-12_13-15-42/global-values.yaml --values /etc/nss/configuration/nss-master-build_3030_2019-10-12_13-15-42/charts/postgres/values.yaml --values /etc/nss/configuration/nss-master-build_3030_2019-10-12_13-15-42/charts/postgres/overrides.yaml

/sgadmin/tools/sgadmin.sh -migrate my_migrate_dir -ks /certificates/admin-keystore.jks -ts /certificates/truststore.jks  -cd /sg-config -tspass ${TRUSTSTORE_PASSWORD} -kspass ${KEYSTORE_PASSWORD} --fail-fast